name: Deploy to AWS

on:
  push:
    branches:
      - main
      - dev
      - 'feature/**'
      - 'bugfix/**'
      - 'hotfix/**'
      - 'test/**'
    paths:
      - 'dashboard/**'
      - '*-instance/**'
      - 'shared/**'
      - '.github/workflows/deploy-dashboard.yml'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: false
        default: 'auto'
        type: choice
        options:
          - auto
          - production
          - staging

env:
  AWS_REGION: ap-southeast-1
  ECR_REPOSITORY: vibe-dashboard
  ECR_CODING_LAB: vibe-coding-lab
  ECS_CLUSTER: vibe-cluster
  ECS_SERVICE: vibe-dashboard
  CONTAINER_NAME: vibe-dashboard

jobs:
  detect:
    name: Detect Instance Changes
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.scan.outputs.matrix }}
      has_instances: ${{ steps.scan.outputs.has_instances }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 50

      - name: Scan for changed instances
        id: scan
        run: |
          MATRIX="[]"

          for dir in *-instance/; do
            [ -f "$dir/Dockerfile" ] || continue

            # Derive tag: strip trailing / and -instance suffix
            TAG="${dir%/}"
            TAG="${TAG%-instance}"

            # Check if this instance changed
            CHANGED=false
            if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
              CHANGED=true
            elif git rev-parse HEAD~1 >/dev/null 2>&1; then
              # For merge commits, HEAD~1 only shows the merge diff.
              # Use --first-parent + diff against HEAD~1 for non-merges,
              # or diff both parents for merge commits to capture all changes.
              if git rev-parse HEAD^2 >/dev/null 2>&1; then
                # Merge commit: diff the merge base of both parents against HEAD
                MERGE_BASE=$(git merge-base HEAD^1 HEAD^2)
                DIFF_FILES=$(git diff --name-only "$MERGE_BASE" HEAD)
              else
                DIFF_FILES=$(git diff --name-only HEAD~1 HEAD)
              fi
              if echo "$DIFF_FILES" | grep -qE "^${dir}|^shared/"; then
                CHANGED=true
              fi
            else
              # No parent commit â€” treat all as changed
              if git ls-files | grep -qE "^${dir}|^shared/"; then
                CHANGED=true
              fi
            fi

            if [ "$CHANGED" == "true" ]; then
              EXTRA=""
              if [ "$TAG" == "continue" ]; then
                EXTRA=",latest"
              fi

              MATRIX=$(echo "$MATRIX" | jq -c \
                --arg dir "${dir%/}" --arg tag "$TAG" --arg extra "$EXTRA" \
                '. += [{"dir": $dir, "tag": $tag, "extra": $extra}]')
            fi
          done

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

          if [ "$MATRIX" == "[]" ]; then
            echo "has_instances=false" >> $GITHUB_OUTPUT
          else
            echo "has_instances=true" >> $GITHUB_OUTPUT
          fi

          echo "Detected instances: $MATRIX"

  build:
    name: Build Images
    runs-on: ubuntu-latest
    outputs:
      dashboard_image: ${{ steps.build-dashboard.outputs.image }}
      environment: ${{ steps.set-env.outputs.environment }}
      environment_name: ${{ steps.set-env.outputs.environment_name }}
      ecs_service: ${{ steps.set-env.outputs.ecs_service }}
      task_family: ${{ steps.set-env.outputs.task_family }}
      tg_name: ${{ steps.set-env.outputs.tg_name }}
      task_cpu: ${{ steps.set-env.outputs.task_cpu }}
      task_memory: ${{ steps.set-env.outputs.task_memory }}
      is_feature_branch: ${{ steps.set-env.outputs.is_feature_branch }}
      branch_slug: ${{ steps.set-env.outputs.branch_slug }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Set environment variables
        id: set-env
        run: |
          BRANCH="${{ github.ref_name }}"

          # Create a slug from branch name (lowercase, replace special chars, truncate)
          # AWS target group names max 32 chars: "vibe-test-" (10) + slug (16) + "-tg" (3) = 29
          BRANCH_SLUG=$(echo "$BRANCH" | sed 's|/|-|g' | sed 's|[^a-zA-Z0-9-]||g' | tr '[:upper:]' '[:lower:]' | cut -c1-16)
          echo "branch_slug=$BRANCH_SLUG" >> $GITHUB_OUTPUT

          # Determine environment type
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ inputs.environment }}" != "auto" ]; then
            # Manual dispatch with explicit environment
            if [ "${{ inputs.environment }}" == "production" ]; then
              ENVIRONMENT="production"
              ENVIRONMENT_NAME="production"
            else
              ENVIRONMENT="staging"
              ENVIRONMENT_NAME="staging"
            fi
            IS_FEATURE="false"
          elif [ "$BRANCH" == "main" ]; then
            ENVIRONMENT="production"
            ENVIRONMENT_NAME="production"
            IS_FEATURE="false"
          elif [ "$BRANCH" == "dev" ]; then
            ENVIRONMENT="staging"
            ENVIRONMENT_NAME="staging"
            IS_FEATURE="false"
          else
            # Feature/test branch - gets its own environment
            ENVIRONMENT="test"
            ENVIRONMENT_NAME="test-$BRANCH_SLUG"
            IS_FEATURE="true"
          fi

          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
          echo "environment_name=$ENVIRONMENT_NAME" >> $GITHUB_OUTPUT
          echo "is_feature_branch=$IS_FEATURE" >> $GITHUB_OUTPUT

          # Set resource names and sizing based on environment
          if [ "$ENVIRONMENT" == "production" ]; then
            echo "ecs_service=vibe-dashboard-service" >> $GITHUB_OUTPUT
            echo "task_family=vibe-dashboard" >> $GITHUB_OUTPUT
            echo "tg_name=vibe-dashboard-tg" >> $GITHUB_OUTPUT
            echo "task_cpu=512" >> $GITHUB_OUTPUT
            echo "task_memory=1024" >> $GITHUB_OUTPUT
          elif [ "$ENVIRONMENT" == "staging" ]; then
            echo "ecs_service=vibe-dashboard-staging" >> $GITHUB_OUTPUT
            echo "task_family=vibe-dashboard-staging" >> $GITHUB_OUTPUT
            echo "tg_name=vibe-dashboard-staging-tg" >> $GITHUB_OUTPUT
            echo "task_cpu=256" >> $GITHUB_OUTPUT
            echo "task_memory=512" >> $GITHUB_OUTPUT
          else
            # Feature branch - minimal resources
            echo "ecs_service=vibe-test-$BRANCH_SLUG" >> $GITHUB_OUTPUT
            echo "task_family=vibe-test-$BRANCH_SLUG" >> $GITHUB_OUTPUT
            echo "tg_name=vibe-test-$BRANCH_SLUG-tg" >> $GITHUB_OUTPUT
            echo "task_cpu=256" >> $GITHUB_OUTPUT
            echo "task_memory=512" >> $GITHUB_OUTPUT
          fi

          echo "Deploying branch '$BRANCH' as environment: $ENVIRONMENT_NAME"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Create ECR repositories and ECS cluster if not exist
        run: |
          aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} 2>/dev/null || \
            aws ecr create-repository --repository-name ${{ env.ECR_REPOSITORY }}
          aws ecr describe-repositories --repository-names ${{ env.ECR_CODING_LAB }} 2>/dev/null || \
            aws ecr create-repository --repository-name ${{ env.ECR_CODING_LAB }}
          aws ecs describe-clusters --clusters ${{ env.ECS_CLUSTER }} --query 'clusters[0].status' --output text 2>/dev/null | grep -q ACTIVE || \
            aws ecs create-cluster --cluster-name ${{ env.ECS_CLUSTER }}

      - name: Build and push Dashboard image
        id: build-dashboard
        uses: docker/build-push-action@v5
        with:
          context: ./dashboard
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}
            ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
          build-args: |
            BUILD_SHA=${{ github.sha }}
          # Disable cache to ensure fresh builds with latest code
          no-cache: true


  build-instances:
    name: Build ${{ matrix.instance.tag }} instance
    needs: [build, detect]
    if: needs.detect.outputs.has_instances == 'true'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        instance: ${{ fromJson(needs.detect.outputs.matrix) }}
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Ensure ECR repository exists
        run: |
          aws ecr describe-repositories --repository-names ${{ env.ECR_CODING_LAB }} 2>/dev/null || \
            aws ecr create-repository --repository-name ${{ env.ECR_CODING_LAB }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Compute image tags
        id: tags
        run: |
          REGISTRY="${{ steps.login-ecr.outputs.registry }}"
          REPO="${{ env.ECR_CODING_LAB }}"
          TAG="${{ matrix.instance.tag }}"
          EXTRA="${{ matrix.instance.extra }}"

          TAGS="${REGISTRY}/${REPO}:${TAG}"

          IFS=',' read -ra EXTRA_TAGS <<< "$EXTRA"
          for et in "${EXTRA_TAGS[@]}"; do
            [ -n "$et" ] && TAGS=$(printf '%s\n%s/%s:%s' "$TAGS" "$REGISTRY" "$REPO" "$et")
          done

          {
            echo "tags<<EOF"
            echo "$TAGS"
            echo "EOF"
          } >> $GITHUB_OUTPUT

      - name: Build and push ${{ matrix.instance.tag }} image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./${{ matrix.instance.dir }}/Dockerfile
          push: true
          tags: ${{ steps.tags.outputs.tags }}
          cache-from: type=gha,scope=${{ matrix.instance.tag }}
          cache-to: type=gha,mode=max,scope=${{ matrix.instance.tag }}

  deploy:
    name: Deploy to ECS
    runs-on: ubuntu-latest
    needs: build

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get infrastructure info
        id: infra
        run: |
          # Get VPC
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=vibe-vpc" --query 'Vpcs[0].VpcId' --output text 2>/dev/null)
          if [ "$VPC_ID" == "None" ] || [ -z "$VPC_ID" ]; then
            VPC_ID=$(aws ec2 describe-vpcs --filters "Name=isDefault,Values=true" --query 'Vpcs[0].VpcId' --output text)
          fi
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT

          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')
          echo "subnet_ids=$SUBNET_IDS" >> $GITHUB_OUTPUT

          # Get existing security groups
          ECS_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-ecs-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          echo "ecs_sg_id=$ECS_SG_ID" >> $GITHUB_OUTPUT

          # Get target group for this environment
          TG_NAME="${{ needs.build.outputs.tg_name }}"
          TG_ARN=$(aws elbv2 describe-target-groups --names $TG_NAME --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          echo "tg_arn=$TG_ARN" >> $GITHUB_OUTPUT
          echo "tg_name=$TG_NAME" >> $GITHUB_OUTPUT

          # Get EFS
          EFS_ID=$(aws efs describe-file-systems --creation-token vibe-dashboard-efs --query "FileSystems[0].FileSystemId" --output text 2>/dev/null || echo "")
          echo "efs_id=$EFS_ID" >> $GITHUB_OUTPUT

          AP_ID=$(aws efs describe-access-points --file-system-id $EFS_ID --query "AccessPoints[0].AccessPointId" --output text 2>/dev/null || echo "")
          echo "ap_id=$AP_ID" >> $GITHUB_OUTPUT

          # Check if first-time setup needed (core infrastructure)
          if [ -z "$ECS_SG_ID" ] || [ "$ECS_SG_ID" == "None" ] || [ -z "$EFS_ID" ] || [ "$EFS_ID" == "None" ] || [ -z "$AP_ID" ] || [ "$AP_ID" == "None" ]; then
            echo "needs_setup=true" >> $GITHUB_OUTPUT
          else
            echo "needs_setup=false" >> $GITHUB_OUTPUT
          fi

          # Check if this specific environment's target group exists
          if [ -z "$TG_ARN" ] || [ "$TG_ARN" == "None" ]; then
            echo "needs_tg_setup=true" >> $GITHUB_OUTPUT
          else
            echo "needs_tg_setup=false" >> $GITHUB_OUTPUT
          fi

      - name: Ensure CI user has CloudFront permissions
        continue-on-error: true
        run: |
          # Get the current IAM user name
          CI_USER=$(aws sts get-caller-identity --query 'Arn' --output text | grep -oP '(?<=user/).*')
          echo "CI user: $CI_USER"
          aws iam put-user-policy --user-name "$CI_USER" --policy-name cloudfront-invalidation --policy-document '{
            "Version": "2012-10-17",
            "Statement": [{
              "Effect": "Allow",
              "Action": [
                "cloudfront:CreateInvalidation",
                "cloudfront:GetInvalidation",
                "cloudfront:ListInvalidations"
              ],
              "Resource": "*"
            }]
          }'
          echo "CloudFront invalidation permissions added to $CI_USER"

      - name: Ensure ecsTaskRole has dashboard permissions
        run: |
          # Ensure ecsTaskRole exists
          aws iam get-role --role-name ecsTaskRole 2>/dev/null || {
            aws iam create-role --role-name ecsTaskRole --assume-role-policy-document '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"ecs-tasks.amazonaws.com"},"Action":"sts:AssumeRole"}]}'
          }

          # Dashboard AWS permissions (for managing coding lab instances)
          # The dashboard uses ECS task role instead of stored credentials
          cat > /tmp/dashboard-policy.json << 'POLICY'
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Sid": "ECSFullAccess",
                "Effect": "Allow",
                "Action": [
                  "ecs:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "EC2ReadAccess",
                "Effect": "Allow",
                "Action": [
                  "ec2:DescribeVpcs",
                  "ec2:DescribeSubnets",
                  "ec2:DescribeSecurityGroups",
                  "ec2:DescribeNetworkInterfaces",
                  "ec2:CreateSecurityGroup",
                  "ec2:AuthorizeSecurityGroupIngress",
                  "ec2:AuthorizeSecurityGroupEgress"
                ],
                "Resource": "*"
              },
              {
                "Sid": "ECRFullAccess",
                "Effect": "Allow",
                "Action": [
                  "ecr:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "IAMRoleManagement",
                "Effect": "Allow",
                "Action": [
                  "iam:GetRole",
                  "iam:CreateRole",
                  "iam:AttachRolePolicy",
                  "iam:PutRolePolicy",
                  "iam:PassRole",
                  "iam:ListRoles"
                ],
                "Resource": "*"
              },
              {
                "Sid": "CloudWatchLogsAccess",
                "Effect": "Allow",
                "Action": [
                  "logs:CreateLogGroup",
                  "logs:CreateLogStream",
                  "logs:PutLogEvents",
                  "logs:DescribeLogGroups",
                  "logs:DescribeLogStreams",
                  "logs:GetLogEvents"
                ],
                "Resource": "*"
              },
              {
                "Sid": "CloudFrontAccess",
                "Effect": "Allow",
                "Action": [
                  "cloudfront:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "CodeBuildAccess",
                "Effect": "Allow",
                "Action": [
                  "codebuild:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "STSAccess",
                "Effect": "Allow",
                "Action": [
                  "sts:GetCallerIdentity"
                ],
                "Resource": "*"
              },
              {
                "Sid": "ELBAccess",
                "Effect": "Allow",
                "Action": [
                  "elasticloadbalancing:*"
                ],
                "Resource": "*"
              }
            ]
          }
          POLICY
          aws iam put-role-policy --role-name ecsTaskRole --policy-name dashboard-aws-access --policy-document file:///tmp/dashboard-policy.json
          echo "Updated ecsTaskRole with dashboard-aws-access policy"

      - name: Checkout for first-time setup
        if: steps.infra.outputs.needs_setup == 'true'
        uses: actions/checkout@v4

      - name: First-time infrastructure setup
        if: steps.infra.outputs.needs_setup == 'true'
        id: setup
        run: |
          VPC_ID="${{ steps.infra.outputs.vpc_id }}"
          SUBNET_IDS="${{ steps.infra.outputs.subnet_ids }}"

          # Create ALB security group
          ALB_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-alb-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          if [ "$ALB_SG_ID" == "None" ] || [ -z "$ALB_SG_ID" ]; then
            ALB_SG_ID=$(aws ec2 create-security-group --group-name vibe-dashboard-alb-sg --description "Vibe Dashboard ALB" --vpc-id $VPC_ID --query 'GroupId' --output text)
            aws ec2 authorize-security-group-ingress --group-id $ALB_SG_ID --protocol tcp --port 80 --cidr 0.0.0.0/0
            aws ec2 authorize-security-group-ingress --group-id $ALB_SG_ID --protocol tcp --port 443 --cidr 0.0.0.0/0
          fi
          echo "alb_sg_id=$ALB_SG_ID" >> $GITHUB_OUTPUT

          # Create ECS security group
          ECS_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-ecs-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          if [ "$ECS_SG_ID" == "None" ] || [ -z "$ECS_SG_ID" ]; then
            ECS_SG_ID=$(aws ec2 create-security-group --group-name vibe-dashboard-ecs-sg --description "Vibe Dashboard ECS" --vpc-id $VPC_ID --query 'GroupId' --output text)
            aws ec2 authorize-security-group-ingress --group-id $ECS_SG_ID --protocol tcp --port 3001 --source-group $ALB_SG_ID
          fi
          echo "ecs_sg_id=$ECS_SG_ID" >> $GITHUB_OUTPUT

          # Create EFS security group
          EFS_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-efs-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          if [ "$EFS_SG_ID" == "None" ] || [ -z "$EFS_SG_ID" ]; then
            EFS_SG_ID=$(aws ec2 create-security-group --group-name vibe-dashboard-efs-sg --description "Vibe Dashboard EFS" --vpc-id $VPC_ID --query 'GroupId' --output text)
            aws ec2 authorize-security-group-ingress --group-id $EFS_SG_ID --protocol tcp --port 2049 --source-group $ECS_SG_ID
          fi

          # Create ALB
          ALB_ARN=$(aws elbv2 describe-load-balancers --names vibe-dashboard-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "None")
          if [ "$ALB_ARN" == "None" ]; then
            SUBNET_LIST=$(echo $SUBNET_IDS | tr ',' ' ')
            ALB_ARN=$(aws elbv2 create-load-balancer --name vibe-dashboard-alb --subnets $SUBNET_LIST --security-groups $ALB_SG_ID --scheme internet-facing --type application --query 'LoadBalancers[0].LoadBalancerArn' --output text)
          fi

          ALB_DNS=$(aws elbv2 describe-load-balancers --load-balancer-arns $ALB_ARN --query 'LoadBalancers[0].DNSName' --output text)
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT

          # Create default target group (for production)
          TG_ARN=$(aws elbv2 describe-target-groups --names vibe-dashboard-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "None")
          if [ "$TG_ARN" == "None" ]; then
            TG_ARN=$(aws elbv2 create-target-group --name vibe-dashboard-tg --protocol HTTP --port 3001 --vpc-id $VPC_ID --target-type ip --health-check-path /api/health --query 'TargetGroups[0].TargetGroupArn' --output text)
          fi
          echo "tg_arn=$TG_ARN" >> $GITHUB_OUTPUT

          # Create listener
          LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn $ALB_ARN --query 'Listeners[?Port==`80`].ListenerArn' --output text 2>/dev/null)
          if [ -z "$LISTENER_ARN" ] || [ "$LISTENER_ARN" == "None" ]; then
            aws elbv2 create-listener --load-balancer-arn $ALB_ARN --protocol HTTP --port 80 --default-actions Type=forward,TargetGroupArn=$TG_ARN
          fi

          # Create EFS
          EFS_ID=$(aws efs describe-file-systems --creation-token vibe-dashboard-efs --query "FileSystems[0].FileSystemId" --output text 2>/dev/null || echo "None")
          if [ -z "$EFS_ID" ] || [ "$EFS_ID" == "None" ]; then
            EFS_ID=$(aws efs create-file-system --creation-token vibe-dashboard-efs --performance-mode generalPurpose --encrypted --query 'FileSystemId' --output text)
            echo "Waiting for EFS $EFS_ID to be available..."
            while true; do
              STATUS=$(aws efs describe-file-systems --file-system-id $EFS_ID --query 'FileSystems[0].LifeCycleState' --output text)
              if [ "$STATUS" == "available" ]; then
                echo "EFS is available"
                break
              fi
              echo "EFS status: $STATUS, waiting..."
              sleep 5
            done
          fi
          echo "efs_id=$EFS_ID" >> $GITHUB_OUTPUT

          # Create mount targets
          for SUBNET in $(echo $SUBNET_IDS | tr ',' ' '); do
            aws efs create-mount-target --file-system-id $EFS_ID --subnet-id $SUBNET --security-groups $EFS_SG_ID 2>/dev/null || true
          done

          # Create access point
          AP_ID=$(aws efs describe-access-points --file-system-id $EFS_ID --query "AccessPoints[0].AccessPointId" --output text 2>/dev/null)
          if [ -z "$AP_ID" ] || [ "$AP_ID" == "None" ]; then
            AP_ID=$(aws efs create-access-point --file-system-id $EFS_ID --posix-user Uid=1000,Gid=1000 --root-directory "Path=/dashboard-data,CreationInfo={OwnerUid=1000,OwnerGid=1000,Permissions=755}" --query 'AccessPointId' --output text)
          fi
          echo "ap_id=$AP_ID" >> $GITHUB_OUTPUT

          # Create IAM roles
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

          aws iam get-role --role-name ecsTaskRole 2>/dev/null || {
            aws iam create-role --role-name ecsTaskRole --assume-role-policy-document '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"ecs-tasks.amazonaws.com"},"Action":"sts:AssumeRole"}]}'
          }

          # EFS access
          aws iam put-role-policy --role-name ecsTaskRole --policy-name efs-access --policy-document "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"elasticfilesystem:ClientMount\",\"elasticfilesystem:ClientWrite\",\"elasticfilesystem:ClientRootAccess\"],\"Resource\":\"arn:aws:elasticfilesystem:${{ env.AWS_REGION }}:${AWS_ACCOUNT_ID}:file-system/${EFS_ID}\"}]}"

          # Dashboard AWS permissions (for managing coding lab instances)
          # The dashboard uses ECS task role instead of stored credentials
          cat > /tmp/dashboard-policy.json << 'POLICY'
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Sid": "ECSFullAccess",
                "Effect": "Allow",
                "Action": [
                  "ecs:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "EC2ReadAccess",
                "Effect": "Allow",
                "Action": [
                  "ec2:DescribeVpcs",
                  "ec2:DescribeSubnets",
                  "ec2:DescribeSecurityGroups",
                  "ec2:DescribeNetworkInterfaces",
                  "ec2:CreateSecurityGroup",
                  "ec2:AuthorizeSecurityGroupIngress",
                  "ec2:AuthorizeSecurityGroupEgress"
                ],
                "Resource": "*"
              },
              {
                "Sid": "ECRFullAccess",
                "Effect": "Allow",
                "Action": [
                  "ecr:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "IAMRoleManagement",
                "Effect": "Allow",
                "Action": [
                  "iam:GetRole",
                  "iam:CreateRole",
                  "iam:AttachRolePolicy",
                  "iam:PutRolePolicy",
                  "iam:PassRole",
                  "iam:ListRoles"
                ],
                "Resource": "*"
              },
              {
                "Sid": "CloudWatchLogsAccess",
                "Effect": "Allow",
                "Action": [
                  "logs:CreateLogGroup",
                  "logs:CreateLogStream",
                  "logs:PutLogEvents",
                  "logs:DescribeLogGroups",
                  "logs:DescribeLogStreams",
                  "logs:GetLogEvents"
                ],
                "Resource": "*"
              },
              {
                "Sid": "CloudFrontAccess",
                "Effect": "Allow",
                "Action": [
                  "cloudfront:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "CodeBuildAccess",
                "Effect": "Allow",
                "Action": [
                  "codebuild:*"
                ],
                "Resource": "*"
              },
              {
                "Sid": "STSAccess",
                "Effect": "Allow",
                "Action": [
                  "sts:GetCallerIdentity"
                ],
                "Resource": "*"
              },
              {
                "Sid": "ELBAccess",
                "Effect": "Allow",
                "Action": [
                  "elasticloadbalancing:*"
                ],
                "Resource": "*"
              }
            ]
          }
          POLICY
          aws iam put-role-policy --role-name ecsTaskRole --policy-name dashboard-aws-access --policy-document file:///tmp/dashboard-policy.json

      - name: Setup environment-specific ALB and target group
        id: env-setup
        run: |
          VPC_ID="${{ steps.infra.outputs.vpc_id }}"
          SUBNET_IDS="${{ steps.infra.outputs.subnet_ids }}"
          ENVIRONMENT="${{ needs.build.outputs.environment }}"
          TG_NAME="${{ needs.build.outputs.tg_name }}"
          BRANCH_SLUG="${{ needs.build.outputs.branch_slug }}"

          # Get or create ALB security group
          ALB_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-alb-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          if [ "$ALB_SG_ID" == "None" ] || [ -z "$ALB_SG_ID" ]; then
            ALB_SG_ID=$(aws ec2 create-security-group --group-name vibe-dashboard-alb-sg --description "Vibe Dashboard ALB" --vpc-id $VPC_ID --query 'GroupId' --output text)
            aws ec2 authorize-security-group-ingress --group-id $ALB_SG_ID --protocol tcp --port 80 --cidr 0.0.0.0/0
            aws ec2 authorize-security-group-ingress --group-id $ALB_SG_ID --protocol tcp --port 443 --cidr 0.0.0.0/0
          fi

          # Determine ALB name based on environment
          if [ "$ENVIRONMENT" == "production" ]; then
            ALB_NAME="vibe-dashboard-alb"
          elif [ "$ENVIRONMENT" == "staging" ]; then
            ALB_NAME="vibe-dashboard-staging-alb"
          else
            # Feature branch gets its own ALB
            ALB_NAME="vibe-test-${BRANCH_SLUG}-alb"
            # Truncate to 32 chars max for ALB name
            ALB_NAME=$(echo "$ALB_NAME" | cut -c1-32)
          fi
          echo "alb_name=$ALB_NAME" >> $GITHUB_OUTPUT

          # Check if ALB exists, create if not
          ALB_ARN=$(aws elbv2 describe-load-balancers --names $ALB_NAME --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "None")

          if [ "$ALB_ARN" == "None" ] || [ -z "$ALB_ARN" ]; then
            echo "Creating ALB: $ALB_NAME"
            SUBNET_LIST=$(echo $SUBNET_IDS | tr ',' ' ')
            ALB_ARN=$(aws elbv2 create-load-balancer \
              --name $ALB_NAME \
              --subnets $SUBNET_LIST \
              --security-groups $ALB_SG_ID \
              --scheme internet-facing \
              --type application \
              --query 'LoadBalancers[0].LoadBalancerArn' --output text)

            echo "Waiting for ALB to be active..."
            aws elbv2 wait load-balancer-available --load-balancer-arns $ALB_ARN
          fi
          echo "alb_arn=$ALB_ARN" >> $GITHUB_OUTPUT

          # Get ALB DNS
          ALB_DNS=$(aws elbv2 describe-load-balancers --load-balancer-arns $ALB_ARN --query 'LoadBalancers[0].DNSName' --output text)
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT

          # Check if target group exists - REUSE if it does, only create if missing
          TG_ARN=$(aws elbv2 describe-target-groups --names $TG_NAME --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "None")

          if [ "$TG_ARN" != "None" ] && [ -n "$TG_ARN" ]; then
            echo "Reusing existing target group: $TG_ARN"

            # Update health check settings on existing target group
            echo "Updating target group health check settings..."
            aws elbv2 modify-target-group \
              --target-group-arn $TG_ARN \
              --health-check-path /api/health \
              --health-check-interval-seconds 30 \
              --health-check-timeout-seconds 10 \
              --healthy-threshold-count 2 \
              --unhealthy-threshold-count 3 || true
          else
            # Create new target group only if it doesn't exist
            echo "Creating target group: $TG_NAME"
            TG_ARN=$(aws elbv2 create-target-group \
              --name $TG_NAME \
              --protocol HTTP --port 3001 \
              --vpc-id $VPC_ID --target-type ip \
              --health-check-path /api/health \
              --health-check-interval-seconds 30 \
              --health-check-timeout-seconds 10 \
              --healthy-threshold-count 2 \
              --unhealthy-threshold-count 3 \
              --query 'TargetGroups[0].TargetGroupArn' --output text)
          fi
          echo "tg_arn=$TG_ARN" >> $GITHUB_OUTPUT

          # Check which ALB the target group is currently associated with (if any)
          TG_ALB_ARNS=$(aws elbv2 describe-target-groups --target-group-arns $TG_ARN --query 'TargetGroups[0].LoadBalancerArns' --output text 2>/dev/null || echo "")

          if [ -n "$TG_ALB_ARNS" ] && [ "$TG_ALB_ARNS" != "None" ]; then
            # Target group is already associated with an ALB
            CURRENT_TG_ALB=$(echo $TG_ALB_ARNS | awk '{print $1}')
            echo "Target group is already associated with ALB: $CURRENT_TG_ALB"

            if [ "$CURRENT_TG_ALB" == "$ALB_ARN" ]; then
              echo "Target group is correctly associated with our ALB"
            else
              # Target group is associated with a different ALB - use that one instead
              echo "Using existing ALB that target group is associated with"
              ALB_ARN=$CURRENT_TG_ALB
              ALB_DNS=$(aws elbv2 describe-load-balancers --load-balancer-arns $ALB_ARN --query 'LoadBalancers[0].DNSName' --output text)
              ALB_NAME=$(aws elbv2 describe-load-balancers --load-balancer-arns $ALB_ARN --query 'LoadBalancers[0].LoadBalancerName' --output text)
              echo "alb_arn=$ALB_ARN" >> $GITHUB_OUTPUT
              echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT
              echo "alb_name=$ALB_NAME" >> $GITHUB_OUTPUT
            fi
          fi

          # Check if listener exists on this ALB
          EXISTING_LISTENER=$(aws elbv2 describe-listeners --load-balancer-arn $ALB_ARN --query 'Listeners[?Port==`80`].ListenerArn' --output text 2>/dev/null || echo "")

          if [ -n "$EXISTING_LISTENER" ] && [ "$EXISTING_LISTENER" != "None" ]; then
            # Check if listener is already pointing to our target group
            LISTENER_TG=$(aws elbv2 describe-listeners --listener-arns $EXISTING_LISTENER --query 'Listeners[0].DefaultActions[0].TargetGroupArn' --output text 2>/dev/null || echo "")

            if [ "$LISTENER_TG" == "$TG_ARN" ]; then
              echo "Listener already configured correctly for target group"
              LISTENER_ARN=$EXISTING_LISTENER
            else
              echo "Updating listener to point to target group: $TG_ARN"
              aws elbv2 modify-listener \
                --listener-arn $EXISTING_LISTENER \
                --default-actions Type=forward,TargetGroupArn=$TG_ARN || true
              LISTENER_ARN=$EXISTING_LISTENER
            fi
          else
            echo "Creating listener for ALB"
            LISTENER_ARN=$(aws elbv2 create-listener \
              --load-balancer-arn $ALB_ARN \
              --protocol HTTP --port 80 \
              --default-actions Type=forward,TargetGroupArn=$TG_ARN \
              --query 'Listeners[0].ListenerArn' --output text)
          fi
          echo "listener_arn=$LISTENER_ARN" >> $GITHUB_OUTPUT

          echo ""
          echo "========================================"
          echo "  ALB Setup Complete"
          echo "  ALB Name: $ALB_NAME"
          echo "  ALB DNS:  $ALB_DNS"
          echo "  Target Group: $TG_NAME"
          echo "========================================"

      - name: Setup CloudFront HTTPS for dashboard
        id: cloudfront
        run: |
          ENVIRONMENT_NAME="${{ needs.build.outputs.environment_name }}"
          ALB_DNS="${{ steps.env-setup.outputs.alb_dns }}"
          CF_COMMENT="Vibe Dashboard - ${ENVIRONMENT_NAME}"

          echo "Checking for existing CloudFront distribution: ${CF_COMMENT}"

          # List all distributions and find one matching our comment tag
          CF_DOMAIN=""
          CF_ID=""
          DIST_LIST=$(aws cloudfront list-distributions --query "DistributionList.Items[?Comment=='${CF_COMMENT}'].[Id,DomainName,Status]" --output text 2>/dev/null || echo "")

          if [ -n "$DIST_LIST" ] && [ "$DIST_LIST" != "None" ]; then
            CF_ID=$(echo "$DIST_LIST" | head -1 | awk '{print $1}')
            CF_DOMAIN=$(echo "$DIST_LIST" | head -1 | awk '{print $2}')
            CF_STATUS=$(echo "$DIST_LIST" | head -1 | awk '{print $3}')
            echo "Found existing CloudFront distribution: $CF_ID ($CF_DOMAIN) - Status: $CF_STATUS"
          fi

          if [ -z "$CF_ID" ]; then
            echo "No existing distribution found. Creating CloudFront distribution for ALB: $ALB_DNS"

            CALLER_REF="vibe-dashboard-${ENVIRONMENT_NAME}-$(date +%s)"

            CF_OUTPUT=$(aws cloudfront create-distribution \
              --distribution-config "{
                \"CallerReference\": \"${CALLER_REF}\",
                \"Comment\": \"${CF_COMMENT}\",
                \"Enabled\": true,
                \"Origins\": {
                  \"Quantity\": 1,
                  \"Items\": [{
                    \"Id\": \"vibe-dashboard-alb\",
                    \"DomainName\": \"${ALB_DNS}\",
                    \"CustomOriginConfig\": {
                      \"HTTPPort\": 80,
                      \"HTTPSPort\": 443,
                      \"OriginProtocolPolicy\": \"http-only\",
                      \"OriginSslProtocols\": { \"Quantity\": 1, \"Items\": [\"TLSv1.2\"] },
                      \"OriginReadTimeout\": 60,
                      \"OriginKeepaliveTimeout\": 60
                    }
                  }]
                },
                \"DefaultCacheBehavior\": {
                  \"TargetOriginId\": \"vibe-dashboard-alb\",
                  \"ViewerProtocolPolicy\": \"redirect-to-https\",
                  \"AllowedMethods\": {
                    \"Quantity\": 7,
                    \"Items\": [\"GET\", \"HEAD\", \"OPTIONS\", \"PUT\", \"POST\", \"PATCH\", \"DELETE\"],
                    \"CachedMethods\": { \"Quantity\": 2, \"Items\": [\"GET\", \"HEAD\"] }
                  },
                  \"CachePolicyId\": \"4135ea2d-6df8-44a3-9df3-4b5a84be39ad\",
                  \"OriginRequestPolicyId\": \"216adef6-5c7f-47e4-b989-5492eafa07d3\",
                  \"Compress\": true
                },
                \"PriceClass\": \"PriceClass_100\",
                \"ViewerCertificate\": { \"CloudFrontDefaultCertificate\": true },
                \"HttpVersion\": \"http2and3\",
                \"IsIPV6Enabled\": true
              }" \
              --query 'Distribution.[Id,DomainName]' --output text 2>&1)

            if [ $? -eq 0 ]; then
              CF_ID=$(echo "$CF_OUTPUT" | awk '{print $1}')
              CF_DOMAIN=$(echo "$CF_OUTPUT" | awk '{print $2}')
              echo "Created CloudFront distribution: $CF_ID ($CF_DOMAIN)"
            else
              echo "WARNING: Failed to create CloudFront distribution: $CF_OUTPUT"
              echo "Falling back to ALB direct access (HTTP)"
            fi
          fi

          echo "cloudfront_domain=$CF_DOMAIN" >> $GITHUB_OUTPUT
          echo "cloudfront_id=$CF_ID" >> $GITHUB_OUTPUT

          if [ -n "$CF_DOMAIN" ]; then
            echo ""
            echo "========================================"
            echo "  CloudFront HTTPS Setup Complete"
            echo "  Distribution: $CF_ID"
            echo "  Domain: $CF_DOMAIN"
            echo "  HTTPS URL: https://$CF_DOMAIN"
            echo "========================================"
          fi

      - name: Get ECR registry
        id: ecr
        run: |
          REGISTRY=$(aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --query 'repositories[0].repositoryUri' --output text | sed 's|/.*||')
          echo "registry=$REGISTRY" >> $GITHUB_OUTPUT

      - name: Register task definition
        id: task-def
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_REGISTRY="${{ steps.ecr.outputs.registry }}"
          ENVIRONMENT="${{ needs.build.outputs.environment }}"
          ENVIRONMENT_NAME="${{ needs.build.outputs.environment_name }}"
          TASK_FAMILY="${{ needs.build.outputs.task_family }}"
          TASK_CPU="${{ needs.build.outputs.task_cpu }}"
          TASK_MEMORY="${{ needs.build.outputs.task_memory }}"
          BRANCH_SLUG="${{ needs.build.outputs.branch_slug }}"

          # Use setup outputs if first-time, otherwise use infra outputs
          if [ "${{ steps.infra.outputs.needs_setup }}" == "true" ]; then
            EFS_ID="${{ steps.setup.outputs.efs_id }}"
            AP_ID="${{ steps.setup.outputs.ap_id }}"
          else
            EFS_ID="${{ steps.infra.outputs.efs_id }}"
            AP_ID="${{ steps.infra.outputs.ap_id }}"
          fi

          cat > task-def.json << EOF
          {
            "family": "${TASK_FAMILY}",
            "networkMode": "awsvpc",
            "requiresCompatibilities": ["FARGATE"],
            "cpu": "${TASK_CPU}",
            "memory": "${TASK_MEMORY}",
            "executionRoleArn": "arn:aws:iam::${AWS_ACCOUNT_ID}:role/ecsTaskExecutionRole",
            "taskRoleArn": "arn:aws:iam::${AWS_ACCOUNT_ID}:role/ecsTaskRole",
            "containerDefinitions": [{
              "name": "${{ env.CONTAINER_NAME }}",
              "image": "${ECR_REGISTRY}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}",
              "essential": true,
              "portMappings": [{"containerPort": 3001, "protocol": "tcp"}],
              "environment": [
                {"name": "NODE_ENV", "value": "production"},
                {"name": "ENVIRONMENT", "value": "${ENVIRONMENT}"},
                {"name": "ENVIRONMENT_NAME", "value": "${ENVIRONMENT_NAME}"},
                {"name": "PORT", "value": "3001"},
                {"name": "DATA_DIR", "value": "/mnt/efs/data/${ENVIRONMENT_NAME}"}
              ],
              "mountPoints": [{"sourceVolume": "efs-data", "containerPath": "/mnt/efs/data", "readOnly": false}],
              "logConfiguration": {
                "logDriver": "awslogs",
                "options": {
                  "awslogs-group": "/ecs/vibe-dashboard",
                  "awslogs-region": "${{ env.AWS_REGION }}",
                  "awslogs-stream-prefix": "ecs",
                  "awslogs-create-group": "true"
                }
              },
              "healthCheck": {
                "command": ["CMD-SHELL", "wget -q --spider http://localhost:3001/api/health || exit 1"],
                "interval": 30, "timeout": 5, "retries": 3, "startPeriod": 60
              }
            }],
            "volumes": [{
              "name": "efs-data",
              "efsVolumeConfiguration": {
                "fileSystemId": "${EFS_ID}",
                "transitEncryption": "ENABLED",
                "authorizationConfig": {"accessPointId": "${AP_ID}", "iam": "ENABLED"}
              }
            }]
          }
          EOF

          TASK_DEF_ARN=$(aws ecs register-task-definition --cli-input-json file://task-def.json --query 'taskDefinition.taskDefinitionArn' --output text)
          echo "task_def_arn=$TASK_DEF_ARN" >> $GITHUB_OUTPUT

      - name: Deploy to ECS (update or create service)
        run: |
          ENVIRONMENT="${{ needs.build.outputs.environment }}"
          ECS_SERVICE_NAME="${{ needs.build.outputs.ecs_service }}"
          TG_ARN="${{ steps.env-setup.outputs.tg_arn }}"

          # Get security group
          if [ "${{ steps.infra.outputs.needs_setup }}" == "true" ]; then
            ECS_SG_ID="${{ steps.setup.outputs.ecs_sg_id }}"
          else
            ECS_SG_ID="${{ steps.infra.outputs.ecs_sg_id }}"
          fi

          SUBNET_IDS="${{ steps.infra.outputs.subnet_ids }}"

          echo "Deploying environment: ${{ needs.build.outputs.environment_name }}"
          echo "Service: $ECS_SERVICE_NAME"
          echo "Target Group: $TG_ARN"

          # Check if service exists and its status
          SERVICE_STATUS=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services $ECS_SERVICE_NAME --query 'services[0].status' --output text 2>/dev/null || echo "MISSING")

          # If service exists, check if its target group matches
          NEEDS_RECREATE="false"
          if [ "$SERVICE_STATUS" == "ACTIVE" ]; then
            CURRENT_TG=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services $ECS_SERVICE_NAME --query 'services[0].loadBalancers[0].targetGroupArn' --output text 2>/dev/null || echo "")
            if [ "$CURRENT_TG" != "$TG_ARN" ]; then
              echo "Target group changed from $CURRENT_TG to $TG_ARN"
              echo "Service must be recreated (can't change load balancer config on existing service)"
              NEEDS_RECREATE="true"
            fi
          fi

          if [ "$SERVICE_STATUS" == "ACTIVE" ] && [ "$NEEDS_RECREATE" == "false" ]; then
            echo "Updating existing service..."
            aws ecs update-service \
              --cluster ${{ env.ECS_CLUSTER }} \
              --service $ECS_SERVICE_NAME \
              --task-definition ${{ steps.task-def.outputs.task_def_arn }} \
              --force-new-deployment
          elif [ "$SERVICE_STATUS" == "ACTIVE" ] && [ "$NEEDS_RECREATE" == "true" ]; then
            echo "Deleting service to recreate with new target group..."
            aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service $ECS_SERVICE_NAME --desired-count 0 2>/dev/null || true
            aws ecs delete-service --cluster ${{ env.ECS_CLUSTER }} --service $ECS_SERVICE_NAME --force
            echo "Waiting for service to be fully deleted..."

            # Wait for service to become INACTIVE (not just drained)
            for i in {1..60}; do
              STATUS=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services $ECS_SERVICE_NAME --query 'services[0].status' --output text 2>/dev/null || echo "MISSING")
              RUNNING=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services $ECS_SERVICE_NAME --query 'services[0].runningCount' --output text 2>/dev/null || echo "0")
              echo "Service status: $STATUS, running tasks: $RUNNING ($i/60)"
              if [ "$STATUS" == "INACTIVE" ] || [ "$STATUS" == "MISSING" ] || [ "$STATUS" == "None" ]; then
                echo "Service is now inactive - ready to create new service"
                break
              fi
              # Keep waiting even if DRAINING with 0 tasks - must wait for INACTIVE
              sleep 5
            done

            echo "Creating new service with new target group..."
            aws ecs create-service \
              --cluster ${{ env.ECS_CLUSTER }} \
              --service-name $ECS_SERVICE_NAME \
              --task-definition ${{ steps.task-def.outputs.task_def_arn }} \
              --desired-count 1 \
              --launch-type FARGATE \
              --network-configuration "awsvpcConfiguration={subnets=[$SUBNET_IDS],securityGroups=[$ECS_SG_ID],assignPublicIp=ENABLED}" \
              --load-balancers "targetGroupArn=$TG_ARN,containerName=${{ env.CONTAINER_NAME }},containerPort=3001" \
              --deployment-configuration "minimumHealthyPercent=0,maximumPercent=200" \
              --health-check-grace-period-seconds 120
          elif [ "$SERVICE_STATUS" == "DRAINING" ]; then
            echo "Service is draining, waiting for it to become inactive..."
            for i in {1..60}; do
              STATUS=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services $ECS_SERVICE_NAME --query 'services[0].status' --output text 2>/dev/null || echo "MISSING")
              RUNNING=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services $ECS_SERVICE_NAME --query 'services[0].runningCount' --output text 2>/dev/null || echo "0")
              echo "Service status: $STATUS, running tasks: $RUNNING ($i/60)"
              if [ "$STATUS" == "INACTIVE" ] || [ "$STATUS" == "MISSING" ] || [ "$STATUS" == "None" ]; then
                echo "Service is now inactive - ready to create new service"
                break
              fi
              # Keep waiting - must reach INACTIVE before creating new service
              sleep 5
            done

            echo "Creating new service..."
            aws ecs create-service \
              --cluster ${{ env.ECS_CLUSTER }} \
              --service-name $ECS_SERVICE_NAME \
              --task-definition ${{ steps.task-def.outputs.task_def_arn }} \
              --desired-count 1 \
              --launch-type FARGATE \
              --network-configuration "awsvpcConfiguration={subnets=[$SUBNET_IDS],securityGroups=[$ECS_SG_ID],assignPublicIp=ENABLED}" \
              --load-balancers "targetGroupArn=$TG_ARN,containerName=${{ env.CONTAINER_NAME }},containerPort=3001" \
              --deployment-configuration "minimumHealthyPercent=0,maximumPercent=200" \
              --health-check-grace-period-seconds 120
          else
            echo "Creating new service..."
            # Delete inactive service if exists
            aws ecs delete-service --cluster ${{ env.ECS_CLUSTER }} --service $ECS_SERVICE_NAME --force 2>/dev/null || true
            sleep 5

            aws ecs create-service \
              --cluster ${{ env.ECS_CLUSTER }} \
              --service-name $ECS_SERVICE_NAME \
              --task-definition ${{ steps.task-def.outputs.task_def_arn }} \
              --desired-count 1 \
              --launch-type FARGATE \
              --network-configuration "awsvpcConfiguration={subnets=[$SUBNET_IDS],securityGroups=[$ECS_SG_ID],assignPublicIp=ENABLED}" \
              --load-balancers "targetGroupArn=$TG_ARN,containerName=${{ env.CONTAINER_NAME }},containerPort=3001" \
              --deployment-configuration "minimumHealthyPercent=0,maximumPercent=200" \
              --health-check-grace-period-seconds 120
          fi

      - name: Invalidate CloudFront cache
        if: steps.cloudfront.outputs.cloudfront_id != ''
        continue-on-error: true
        run: |
          CF_ID="${{ steps.cloudfront.outputs.cloudfront_id }}"
          echo "Invalidating CloudFront cache for distribution: $CF_ID"
          aws cloudfront create-invalidation \
            --distribution-id "$CF_ID" \
            --paths "/*" \
            --query 'Invalidation.Id' --output text
          echo "Cache invalidation submitted"

      - name: Output URLs
        run: |
          ENVIRONMENT="${{ needs.build.outputs.environment }}"
          ENVIRONMENT_NAME="${{ needs.build.outputs.environment_name }}"
          ECS_SERVICE_NAME="${{ needs.build.outputs.ecs_service }}"
          BRANCH_SLUG="${{ needs.build.outputs.branch_slug }}"
          ALB_DNS="${{ steps.env-setup.outputs.alb_dns }}"
          ALB_NAME="${{ steps.env-setup.outputs.alb_name }}"
          CF_DOMAIN="${{ steps.cloudfront.outputs.cloudfront_domain }}"

          # Use CloudFront HTTPS URL if available, otherwise fall back to ALB HTTP
          if [ -n "$CF_DOMAIN" ]; then
            BASE_URL="https://$CF_DOMAIN"
            URL_NOTE="(via CloudFront HTTPS)"
          else
            BASE_URL="http://$ALB_DNS"
            URL_NOTE="(direct ALB - HTTP only)"
          fi

          echo ""
          echo "========================================"
          echo "  DEPLOYMENT COMPLETE!"
          echo "========================================"
          echo ""
          echo "  Environment: $ENVIRONMENT_NAME"
          echo "  Service: $ECS_SERVICE_NAME"
          echo "  Branch: ${{ github.ref_name }}"
          echo "  ALB: $ALB_NAME"
          echo ""
          echo "  URLs ${URL_NOTE}:"
          echo "    Dashboard:    $BASE_URL"
          echo "    Admin Login:  $BASE_URL/#/login"
          echo "    Admin Panel:  $BASE_URL/#/admin"
          echo "    Participant:  $BASE_URL/#/enter"
          echo "    Health Check: $BASE_URL/api/health"
          echo "    Version:      $BASE_URL/api/version"
          echo ""
          if [ -n "$CF_DOMAIN" ]; then
            echo "  CloudFront: ${{ steps.cloudfront.outputs.cloudfront_id }}"
            echo "  ALB (origin): http://$ALB_DNS"
          fi
          if [ "$ENVIRONMENT" == "test" ]; then
            echo "  Note: This is a feature branch deployment with its own ALB."
            echo "  Main branch (production) remains unaffected at vibe-dashboard-alb"
          fi
          echo "========================================"
