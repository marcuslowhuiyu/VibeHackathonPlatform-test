name: Deploy to AWS

on:
  push:
    branches: [main]
    paths:
      - 'dashboard/**'
      - 'cline-setup/**'
      - '.github/workflows/deploy-dashboard.yml'
  workflow_dispatch: # Allow manual trigger from GitHub UI

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: vibe-dashboard
  ECR_CODING_LAB: vibe-coding-lab
  ECS_CLUSTER: vibe-cluster
  ECS_SERVICE: vibe-dashboard
  CONTAINER_NAME: vibe-dashboard

jobs:
  deploy:
    name: Build and Deploy
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repository if not exists
        run: |
          aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} 2>/dev/null || \
          aws ecr create-repository --repository-name ${{ env.ECR_REPOSITORY }}

      - name: Build, tag, and push Dashboard image to ECR
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          cd dashboard
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Create Coding Lab ECR repository if not exists
        run: |
          aws ecr describe-repositories --repository-names ${{ env.ECR_CODING_LAB }} 2>/dev/null || \
          aws ecr create-repository --repository-name ${{ env.ECR_CODING_LAB }}

      - name: Build, tag, and push Coding Lab image to ECR
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          cd cline-setup
          echo "Building coding lab image with Continue AI..."
          docker build --build-arg AI_EXTENSION=continue -t $ECR_REGISTRY/$ECR_CODING_LAB:continue .
          docker tag $ECR_REGISTRY/$ECR_CODING_LAB:continue $ECR_REGISTRY/$ECR_CODING_LAB:latest
          echo "Pushing to ECR..."
          docker push $ECR_REGISTRY/$ECR_CODING_LAB:continue
          docker push $ECR_REGISTRY/$ECR_CODING_LAB:latest
          echo "Coding lab image pushed: $ECR_REGISTRY/$ECR_CODING_LAB:latest"

      - name: Setup Infrastructure (VPC, ALB, Target Group)
        id: infra
        run: |
          # Get VPC
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=vibe-vpc" --query 'Vpcs[0].VpcId' --output text)
          if [ "$VPC_ID" == "None" ] || [ -z "$VPC_ID" ]; then
            VPC_ID=$(aws ec2 describe-vpcs --filters "Name=isDefault,Values=true" --query 'Vpcs[0].VpcId' --output text)
          fi
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT

          # Get subnets (need at least 2 in different AZs for ALB)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')
          echo "subnet_ids=$SUBNET_IDS" >> $GITHUB_OUTPUT

          # Create or get ALB security group
          ALB_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-alb-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          if [ "$ALB_SG_ID" == "None" ] || [ -z "$ALB_SG_ID" ]; then
            ALB_SG_ID=$(aws ec2 create-security-group --group-name vibe-dashboard-alb-sg --description "Vibe Dashboard ALB Security Group" --vpc-id $VPC_ID --query 'GroupId' --output text)
            aws ec2 authorize-security-group-ingress --group-id $ALB_SG_ID --protocol tcp --port 80 --cidr 0.0.0.0/0
            aws ec2 authorize-security-group-ingress --group-id $ALB_SG_ID --protocol tcp --port 443 --cidr 0.0.0.0/0
          fi
          echo "alb_sg_id=$ALB_SG_ID" >> $GITHUB_OUTPUT

          # Create or get ECS task security group
          ECS_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-ecs-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          if [ "$ECS_SG_ID" == "None" ] || [ -z "$ECS_SG_ID" ]; then
            ECS_SG_ID=$(aws ec2 create-security-group --group-name vibe-dashboard-ecs-sg --description "Vibe Dashboard ECS Security Group" --vpc-id $VPC_ID --query 'GroupId' --output text)
            # Allow traffic from ALB only
            aws ec2 authorize-security-group-ingress --group-id $ECS_SG_ID --protocol tcp --port 3001 --source-group $ALB_SG_ID
          fi
          echo "ecs_sg_id=$ECS_SG_ID" >> $GITHUB_OUTPUT

          # Create or get EFS security group
          EFS_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=vibe-dashboard-efs-sg" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)
          if [ "$EFS_SG_ID" == "None" ] || [ -z "$EFS_SG_ID" ]; then
            EFS_SG_ID=$(aws ec2 create-security-group --group-name vibe-dashboard-efs-sg --description "Vibe Dashboard EFS Security Group" --vpc-id $VPC_ID --query 'GroupId' --output text)
            # Allow NFS from ECS tasks
            aws ec2 authorize-security-group-ingress --group-id $EFS_SG_ID --protocol tcp --port 2049 --source-group $ECS_SG_ID
          fi
          echo "efs_sg_id=$EFS_SG_ID" >> $GITHUB_OUTPUT

          # Create or get ALB
          ALB_ARN=$(aws elbv2 describe-load-balancers --names vibe-dashboard-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "None")
          if [ "$ALB_ARN" == "None" ] || [ -z "$ALB_ARN" ]; then
            # Convert comma-separated to space-separated for ALB creation
            SUBNET_LIST=$(echo $SUBNET_IDS | tr ',' ' ')
            ALB_ARN=$(aws elbv2 create-load-balancer \
              --name vibe-dashboard-alb \
              --subnets $SUBNET_LIST \
              --security-groups $ALB_SG_ID \
              --scheme internet-facing \
              --type application \
              --query 'LoadBalancers[0].LoadBalancerArn' \
              --output text)
            echo "Created new ALB: $ALB_ARN"
          fi
          echo "alb_arn=$ALB_ARN" >> $GITHUB_OUTPUT

          # Get ALB DNS name
          ALB_DNS=$(aws elbv2 describe-load-balancers --load-balancer-arns $ALB_ARN --query 'LoadBalancers[0].DNSName' --output text)
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT

          # Create or get target group
          TG_ARN=$(aws elbv2 describe-target-groups --names vibe-dashboard-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "None")
          if [ "$TG_ARN" == "None" ] || [ -z "$TG_ARN" ]; then
            TG_ARN=$(aws elbv2 create-target-group \
              --name vibe-dashboard-tg \
              --protocol HTTP \
              --port 3001 \
              --vpc-id $VPC_ID \
              --target-type ip \
              --health-check-path /api/health \
              --health-check-interval-seconds 30 \
              --healthy-threshold-count 2 \
              --unhealthy-threshold-count 3 \
              --query 'TargetGroups[0].TargetGroupArn' \
              --output text)
            echo "Created new target group: $TG_ARN"
          fi
          echo "tg_arn=$TG_ARN" >> $GITHUB_OUTPUT

          # Create HTTP listener if not exists
          LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn $ALB_ARN --query 'Listeners[?Port==`80`].ListenerArn' --output text 2>/dev/null)
          if [ -z "$LISTENER_ARN" ] || [ "$LISTENER_ARN" == "None" ]; then
            aws elbv2 create-listener \
              --load-balancer-arn $ALB_ARN \
              --protocol HTTP \
              --port 80 \
              --default-actions Type=forward,TargetGroupArn=$TG_ARN
            echo "Created HTTP listener"
          fi

      - name: Setup EFS for persistent storage
        id: efs
        run: |
          VPC_ID="${{ steps.infra.outputs.vpc_id }}"
          SUBNET_IDS="${{ steps.infra.outputs.subnet_ids }}"
          EFS_SG_ID="${{ steps.infra.outputs.efs_sg_id }}"

          # Create or get EFS file system (using creation-token to find existing)
          EFS_ID=$(aws efs describe-file-systems --creation-token vibe-dashboard-efs --query "FileSystems[0].FileSystemId" --output text 2>/dev/null || echo "None")
          if [ -z "$EFS_ID" ] || [ "$EFS_ID" == "None" ]; then
            echo "Creating new EFS file system..."
            EFS_ID=$(aws efs create-file-system \
              --creation-token vibe-dashboard-efs \
              --performance-mode generalPurpose \
              --throughput-mode bursting \
              --encrypted \
              --query 'FileSystemId' --output text)

            # Wait for EFS to be available
            echo "Waiting for EFS to be available..."
            while [ "$(aws efs describe-file-systems --file-system-id $EFS_ID --query 'FileSystems[0].LifeCycleState' --output text)" != "available" ]; do
              sleep 5
            done
            echo "EFS is available: $EFS_ID"
          else
            echo "Using existing EFS: $EFS_ID"
          fi
          echo "efs_id=$EFS_ID" >> $GITHUB_OUTPUT

          # Create mount targets in each subnet
          for SUBNET in $(echo $SUBNET_IDS | tr ',' ' '); do
            MT_ID=$(aws efs describe-mount-targets --file-system-id $EFS_ID --query "MountTargets[?SubnetId=='$SUBNET'].MountTargetId" --output text 2>/dev/null)
            if [ -z "$MT_ID" ] || [ "$MT_ID" == "None" ]; then
              echo "Creating mount target in subnet $SUBNET..."
              aws efs create-mount-target \
                --file-system-id $EFS_ID \
                --subnet-id $SUBNET \
                --security-groups $EFS_SG_ID || true
            else
              echo "Mount target already exists in subnet $SUBNET"
            fi
          done

          # Wait for mount targets to be available
          echo "Waiting for mount targets..."
          sleep 30

          # Create or get access point
          AP_ID=$(aws efs describe-access-points --file-system-id $EFS_ID --query "AccessPoints[0].AccessPointId" --output text 2>/dev/null)
          if [ -z "$AP_ID" ] || [ "$AP_ID" == "None" ]; then
            echo "Creating access point..."
            AP_ID=$(aws efs create-access-point \
              --file-system-id $EFS_ID \
              --posix-user Uid=1000,Gid=1000 \
              --root-directory "Path=/dashboard-data,CreationInfo={OwnerUid=1000,OwnerGid=1000,Permissions=755}" \
              --query 'AccessPointId' --output text)
            echo "Created access point: $AP_ID"
          else
            echo "Using existing access point: $AP_ID"
          fi
          echo "ap_id=$AP_ID" >> $GITHUB_OUTPUT

      - name: Setup ECS Task Role for EFS
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          EFS_ID="${{ steps.efs.outputs.efs_id }}"

          # Check if task role exists
          TASK_ROLE_EXISTS=$(aws iam get-role --role-name ecsTaskRole 2>/dev/null && echo "yes" || echo "no")

          if [ "$TASK_ROLE_EXISTS" == "no" ]; then
            echo "Creating ecsTaskRole..."
            cat > trust-policy.json << 'EOF'
          {
            "Version": "2012-10-17",
            "Statement": [{
              "Effect": "Allow",
              "Principal": {"Service": "ecs-tasks.amazonaws.com"},
              "Action": "sts:AssumeRole"
            }]
          }
          EOF
            aws iam create-role --role-name ecsTaskRole --assume-role-policy-document file://trust-policy.json
          fi

          # Attach EFS policy
          echo "Attaching EFS access policy..."
          cat > efs-policy.json << EOF
          {
            "Version": "2012-10-17",
            "Statement": [{
              "Effect": "Allow",
              "Action": [
                "elasticfilesystem:ClientMount",
                "elasticfilesystem:ClientWrite",
                "elasticfilesystem:ClientRootAccess"
              ],
              "Resource": "arn:aws:elasticfilesystem:${{ env.AWS_REGION }}:${AWS_ACCOUNT_ID}:file-system/${EFS_ID}"
            }]
          }
          EOF
          aws iam put-role-policy --role-name ecsTaskRole --policy-name efs-access --policy-document file://efs-policy.json

      - name: Register new task definition
        id: task-def
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          EFS_ID="${{ steps.efs.outputs.efs_id }}"
          AP_ID="${{ steps.efs.outputs.ap_id }}"

          # Create task definition JSON with EFS volume
          cat > task-def.json << EOF
          {
            "family": "vibe-dashboard",
            "networkMode": "awsvpc",
            "requiresCompatibilities": ["FARGATE"],
            "cpu": "512",
            "memory": "1024",
            "executionRoleArn": "arn:aws:iam::${AWS_ACCOUNT_ID}:role/ecsTaskExecutionRole",
            "taskRoleArn": "arn:aws:iam::${AWS_ACCOUNT_ID}:role/ecsTaskRole",
            "containerDefinitions": [
              {
                "name": "${{ env.CONTAINER_NAME }}",
                "image": "${ECR_REGISTRY}/${{ env.ECR_REPOSITORY }}:${IMAGE_TAG}",
                "essential": true,
                "portMappings": [
                  {
                    "containerPort": 3001,
                    "protocol": "tcp"
                  }
                ],
                "environment": [
                  {"name": "NODE_ENV", "value": "production"},
                  {"name": "PORT", "value": "3001"},
                  {"name": "DATA_DIR", "value": "/mnt/efs/data"}
                ],
                "mountPoints": [
                  {
                    "sourceVolume": "efs-data",
                    "containerPath": "/mnt/efs/data",
                    "readOnly": false
                  }
                ],
                "logConfiguration": {
                  "logDriver": "awslogs",
                  "options": {
                    "awslogs-group": "/ecs/vibe-dashboard",
                    "awslogs-region": "${{ env.AWS_REGION }}",
                    "awslogs-stream-prefix": "ecs",
                    "awslogs-create-group": "true"
                  }
                },
                "healthCheck": {
                  "command": ["CMD-SHELL", "wget -q --spider http://localhost:3001/api/health || exit 1"],
                  "interval": 30,
                  "timeout": 5,
                  "retries": 3,
                  "startPeriod": 60
                }
              }
            ],
            "volumes": [
              {
                "name": "efs-data",
                "efsVolumeConfiguration": {
                  "fileSystemId": "${EFS_ID}",
                  "transitEncryption": "ENABLED",
                  "authorizationConfig": {
                    "accessPointId": "${AP_ID}",
                    "iam": "ENABLED"
                  }
                }
              }
            ]
          }
          EOF

          # Register the task definition
          TASK_DEF_ARN=$(aws ecs register-task-definition --cli-input-json file://task-def.json --query 'taskDefinition.taskDefinitionArn' --output text)
          echo "task_def_arn=$TASK_DEF_ARN" >> $GITHUB_OUTPUT

      - name: Delete existing ECS service
        run: |
          # Check if service exists and delete it
          SERVICE_STATUS=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services ${{ env.ECS_SERVICE }} --query 'services[0].status' --output text 2>/dev/null || echo "MISSING")

          if [ "$SERVICE_STATUS" == "ACTIVE" ]; then
            echo "Deleting existing service..."
            aws ecs delete-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --force

            # Wait for service to be fully deleted
            echo "Waiting for service to be deleted..."
            while true; do
              STATUS=$(aws ecs describe-services --cluster ${{ env.ECS_CLUSTER }} --services ${{ env.ECS_SERVICE }} --query 'services[0].status' --output text 2>/dev/null || echo "MISSING")
              if [ "$STATUS" == "INACTIVE" ] || [ "$STATUS" == "MISSING" ]; then
                echo "Service deleted successfully"
                break
              fi
              echo "Current status: $STATUS - waiting..."
              sleep 10
            done
          else
            echo "No active service found, proceeding with creation"
          fi

      - name: Create ECS service
        run: |
          # Create the ECS service with ALB integration
          aws ecs create-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service-name ${{ env.ECS_SERVICE }} \
            --task-definition ${{ steps.task-def.outputs.task_def_arn }} \
            --desired-count 1 \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ steps.infra.outputs.subnet_ids }}],securityGroups=[${{ steps.infra.outputs.ecs_sg_id }}],assignPublicIp=ENABLED}" \
            --load-balancers "targetGroupArn=${{ steps.infra.outputs.tg_arn }},containerName=${{ env.CONTAINER_NAME }},containerPort=3001" \
            --deployment-configuration "minimumHealthyPercent=0,maximumPercent=200" \
            --health-check-grace-period-seconds 60

      - name: Setup CloudFront Distribution
        id: cloudfront
        run: |
          ALB_DNS="${{ steps.infra.outputs.alb_dns }}"

          # Check if CloudFront distribution exists for this ALB
          EXISTING_DIST=$(aws cloudfront list-distributions --query "DistributionList.Items[?Origins.Items[0].DomainName=='$ALB_DNS'].Id" --output text 2>/dev/null || echo "")

          if [ -n "$EXISTING_DIST" ] && [ "$EXISTING_DIST" != "None" ]; then
            echo "CloudFront distribution already exists: $EXISTING_DIST"
            CF_DOMAIN=$(aws cloudfront get-distribution --id $EXISTING_DIST --query 'Distribution.DomainName' --output text)
            echo "cf_domain=$CF_DOMAIN" >> $GITHUB_OUTPUT
            echo "cf_id=$EXISTING_DIST" >> $GITHUB_OUTPUT
          else
            # Create CloudFront distribution
            CALLER_REF="vibe-dashboard-$(date +%s)"

            cat > cf-config.json << EOF
          {
            "CallerReference": "$CALLER_REF",
            "Comment": "Vibe Dashboard HTTPS",
            "Enabled": true,
            "Origins": {
              "Quantity": 1,
              "Items": [
                {
                  "Id": "alb-origin",
                  "DomainName": "$ALB_DNS",
                  "CustomOriginConfig": {
                    "HTTPPort": 80,
                    "HTTPSPort": 443,
                    "OriginProtocolPolicy": "http-only",
                    "OriginSslProtocols": {
                      "Quantity": 1,
                      "Items": ["TLSv1.2"]
                    }
                  }
                }
              ]
            },
            "DefaultCacheBehavior": {
              "TargetOriginId": "alb-origin",
              "ViewerProtocolPolicy": "redirect-to-https",
              "AllowedMethods": {
                "Quantity": 7,
                "Items": ["GET", "HEAD", "OPTIONS", "PUT", "POST", "PATCH", "DELETE"],
                "CachedMethods": {
                  "Quantity": 2,
                  "Items": ["GET", "HEAD"]
                }
              },
              "ForwardedValues": {
                "QueryString": true,
                "Cookies": {
                  "Forward": "all"
                },
                "Headers": {
                  "Quantity": 3,
                  "Items": ["Authorization", "Origin", "Host"]
                }
              },
              "MinTTL": 0,
              "DefaultTTL": 0,
              "MaxTTL": 0,
              "Compress": true
            },
            "PriceClass": "PriceClass_100",
            "ViewerCertificate": {
              "CloudFrontDefaultCertificate": true
            }
          }
          EOF

            CF_RESULT=$(aws cloudfront create-distribution --distribution-config file://cf-config.json)
            CF_ID=$(echo $CF_RESULT | jq -r '.Distribution.Id')
            CF_DOMAIN=$(echo $CF_RESULT | jq -r '.Distribution.DomainName')

            echo "Created CloudFront distribution: $CF_ID"
            echo "cf_domain=$CF_DOMAIN" >> $GITHUB_OUTPUT
            echo "cf_id=$CF_ID" >> $GITHUB_OUTPUT
          fi

      - name: Wait for service stability
        run: |
          echo "Waiting for service to stabilize..."
          aws ecs wait services-stable --cluster ${{ env.ECS_CLUSTER }} --services ${{ env.ECS_SERVICE }} || true

      - name: Output URLs
        run: |
          echo ""
          echo "========================================"
          echo "  DEPLOYMENT COMPLETE!"
          echo "========================================"
          echo ""
          echo "  HTTPS URLs (Recommended):"
          echo "  -------------------------"
          echo "  Admin Login:        https://${{ steps.cloudfront.outputs.cf_domain }}/#/login"
          echo "  Participant Portal: https://${{ steps.cloudfront.outputs.cf_domain }}/#/portal"
          echo ""
          echo "  HTTP URLs (Direct ALB - not secure):"
          echo "  ------------------------------------"
          echo "  Admin Login:        http://${{ steps.infra.outputs.alb_dns }}/#/login"
          echo "  Participant Portal: http://${{ steps.infra.outputs.alb_dns }}/#/portal"
          echo ""
          echo "  CloudFront ID: ${{ steps.cloudfront.outputs.cf_id }}"
          echo ""
          echo "  NOTE: CloudFront may take 5-15 minutes to fully deploy."
          echo "        Use the ALB URL initially if CloudFront isn't ready."
          echo ""
          echo "  To add a custom domain later:"
          echo "  1. Request ACM certificate in us-east-1"
          echo "  2. Add domain to CloudFront alternate domain names"
          echo "  3. Create Route 53 alias record pointing to CloudFront"
          echo ""
          echo "========================================"
